{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8ca519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretical question \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411f26a",
   "metadata": {},
   "source": [
    "Q1) \n",
    "\n",
    "Deep Learning is a subfield of Machine Learning, which itself is a branch of Artificial Intelligence (AI). It uses neural networks with multiple layers to learn representations of data. Deep learning is used in tasks like speech recognition, image classification, NLP, and autonomous driving — all key AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07af575",
   "metadata": {},
   "source": [
    "Q2 ) \n",
    "\n",
    "A neural network mimics the human brain, consisting of interconnected nodes (neurons). It maps input data to outputs via layers.\n",
    "\n",
    "Types:\n",
    "\n",
    "Feedforward Neural Network (FNN) – Basic model, data flows one way.\n",
    "\n",
    "Convolutional Neural Network (CNN) – Best for image processing.\n",
    "\n",
    "Recurrent Neural Network (RNN) – Best for sequences (e.g., text, time-series).\n",
    "\n",
    "Autoencoders – For compression and denoising.\n",
    "\n",
    "GANs – Generative Adversarial Networks for image/video generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7ee9b",
   "metadata": {},
   "source": [
    "Q3 ) \n",
    "Each layer performs a linear transformation followed by an activation:\n",
    "\n",
    "Z = W·X + b\n",
    "A = f(Z)\n",
    "\n",
    "Where:\n",
    "\n",
    "W = weights,\n",
    "\n",
    "X = input,\n",
    "\n",
    "b = bias,\n",
    "\n",
    "f = activation function (like ReLU),\n",
    "\n",
    "A = output of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25085c87",
   "metadata": {},
   "source": [
    "Q4 ) An activation function adds non-linearity so the network can learn complex patterns. Without it, the network behaves like a linear regression model, no matter how many layers you add.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582d05f",
   "metadata": {},
   "source": [
    "Q5) \n",
    "Yes:\n",
    "\n",
    "ReLU (Rectified Linear Unit): max(0, x) — Fast and widely used.\n",
    "\n",
    "Sigmoid: Squashes output between 0 and 1 — for binary classification.\n",
    "\n",
    "Tanh: Like sigmoid, but ranges from -1 to 1 — centered.\n",
    "\n",
    "Softmax: Converts logits into probabilities — for multi-class classification.\n",
    "\n",
    "Leaky ReLU: Avoids ReLU’s dying neuron problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1aa3fb",
   "metadata": {},
   "source": [
    "Q6)  \n",
    "A neural network with multiple hidden layers between input and output is called a multilayer network or deep neural network. These networks can learn highly complex relationships.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14743476",
   "metadata": {},
   "source": [
    "Q7 ) \n",
    "\n",
    "A loss function quantifies how well the model’s predictions match the actual labels. It guides the training by generating gradients during backpropagation. Lower loss = better model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f692c4",
   "metadata": {},
   "source": [
    "Q8 ) \n",
    "\n",
    "Mean Squared Error (MSE) – Regression.\n",
    "\n",
    "Binary Cross-Entropy – Binary classification.\n",
    "\n",
    "Categorical Cross-Entropy – Multi-class classification.\n",
    "\n",
    "Hinge Loss – For SVM-like classification tasks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891f266",
   "metadata": {},
   "source": [
    "Q9 ) \n",
    "Forward propagation: Input flows through layers to produce output.\n",
    "\n",
    "Loss calculation: Compare predicted vs. actual.\n",
    "\n",
    "Backward propagation: Compute gradients of loss w.r.t. weights.\n",
    "\n",
    "Weight update: Use gradient descent or optimizers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e04343",
   "metadata": {},
   "source": [
    "Q10 ) \n",
    "\n",
    "\n",
    "An optimizer adjusts weights to minimize the loss function. It uses gradients from backpropagation to improve the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8539c10",
   "metadata": {},
   "source": [
    "Q11 ) \n",
    "\n",
    "SGD (Stochastic Gradient Descent) – Basic, noisy updates.\n",
    "\n",
    "Momentum – Adds inertia to SGD.\n",
    "\n",
    "RMSProp – Uses decaying average of squared gradients.\n",
    "\n",
    "Adam – Combines momentum + RMSProp (most commonly used).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227b3e8",
   "metadata": {},
   "source": [
    "Q12 )\n",
    "\n",
    "Forward Propagation: Data moves layer by layer to make a prediction.\n",
    "\n",
    "Backward Propagation: Loss is propagated back, computing gradients w.r.t. each weight using the chain rule. These gradients are then used to update weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e08759",
   "metadata": {},
   "source": [
    "Q13 ) \n",
    "\n",
    "\n",
    "Weight initialization sets initial values for weights. Good initialization:\n",
    "\n",
    "Avoids vanishing/exploding gradients.\n",
    "\n",
    "Speeds up convergence.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Xavier (Glorot): For tanh/sigmoid.\n",
    "\n",
    "He initialization: For ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684d927",
   "metadata": {},
   "source": [
    "Q14 ) \n",
    "\n",
    "In deep networks, gradients can become very small, especially with sigmoid/tanh activations, making early layers learn very slowly. This leads to poor performance and convergence issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f458c",
   "metadata": {},
   "source": [
    "Q15 ) \n",
    "\n",
    "Gradients become too large, especially in deep networks or RNNs, leading to unstable updates and NaN losses. It can be controlled using gradient clipping.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925f6f2",
   "metadata": {},
   "source": [
    "PRACTICAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1673de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q1 \n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(1, input_shape=(n_features,), activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ca063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q2 )  \n",
    "\n",
    "\n",
    "# model = Sequential([\n",
    "#     Dense(16, input_shape=(n_features,), activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442c8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q3 ) \n",
    "# from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "# model.add(Dense(16, activation='relu', kernel_initializer=GlorotUniform()))\n",
    "# # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q4 ) \n",
    "\n",
    "\n",
    "# model = Sequential([\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(32, activation='tanh'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91780aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q5 ) \n",
    "\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# model = Sequential([\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q6 ) \n",
    "# import numpy as np\n",
    "\n",
    "# def relu(x):\n",
    "#     return np.maximum(0, x)\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# # Sample weights, biases, input\n",
    "# W1 = np.random.randn(4, 3)\n",
    "# b1 = np.zeros((4, 1))\n",
    "# W2 = np.random.randn(1, 4)\n",
    "# b2 = np.zeros((1, 1))\n",
    "\n",
    "# X = np.random.randn(3, 1)\n",
    "\n",
    "# # Forward Propagation\n",
    "# Z1 = np.dot(W1, X) + b1\n",
    "# A1 = relu(Z1)\n",
    "# Z2 = np.dot(W2, A1) + b2\n",
    "# A2 = sigmoid(Z2)\n",
    "\n",
    "# print(\"Output:\", A2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fd48097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q7 ) \n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# model = Sequential([\n",
    "#     Dense(64, activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cbdca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q8 ) \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "\n",
    "# # Plot\n",
    "# plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "# plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "# plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Training Progress')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eaa0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q9)\n",
    "\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# opt = Adam(clipvalue=1.0)\n",
    "# model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef5d64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q10)\n",
    "\n",
    "# import tensorflow.keras.backend as K\n",
    "\n",
    "# def custom_mse_loss(y_true, y_pred):\n",
    "#     return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "# model.compile(optimizer='adam', loss=custom_mse_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba0ac2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q11)\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
